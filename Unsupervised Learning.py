# -*- coding: utf-8 -*-
"""Unsupervised Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wbaGewMi7G91q1kFIZglaailq3uQnpBO
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install pandas
!pip install scipy
!pip install matplotlib
!pip install scikit-learn
!pip install seaborn

import pandas as pd
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import *
from matplotlib import pyplot as plt
from matplotlib import rc
import numpy as np
from sklearn.cluster import KMeans
# %matplotlib inline

df=pd.read_csv('C:/Users/lizab/Downloads/diamonds.csv')

table=df.sample(5)
table.to_csv('table.csv')
df.sample(5)

df.drop (df.columns [[0]], axis= 1 , inplace= True )

float64_cols = list(df.select_dtypes(include='float64'))
df[float64_cols] = df[float64_cols].astype('float32')

df1 = df.sample (n=1000) #выбрали случайным образом 1000 наблюдений, выборку пришлось уменьшить из-за возникающих проблем при большом наборе данных

int64_cols = list(df.select_dtypes(include='int64'))
df[int64_cols] = df[int64_cols].astype('int32')

col=['carat', 'price','x','y','z']

from pandas.plotting import scatter_matrix
scatter_matrix(df1[col], alpha=0.05, figsize=(10, 10))

corr=df1[col].corr() #парные корреляции
corr.to_csv('corr.csv')

from sklearn import preprocessing
dataNorm = preprocessing.MinMaxScaler().fit_transform(df1[col].values)

#Евклидово расстояние между каждым набором данных
data_dist = pdist(dataNorm, 'euclidean')
#Функция для визуализации и выделения количества кластеров
data_linkage = linkage(data_dist, method='average')

# Метод локтя. Позволячет оценить оптимальное количество кластеров.
last = data_linkage[-10:, 2]
last_rev = last[::-1]
idxs = np.arange(1, len(last) + 1)
plt.plot(idxs, last_rev)

acceleration = np.diff(last, 2)
acceleration_rev = acceleration[::-1]
plt.plot(idxs[:-2] + 1, acceleration_rev)
plt.show()
k = acceleration_rev.argmax() + 2
print("Рекомендованное количество кластеров:", k)

nClust=5 #Количество кластеров для использования

fancy_dendrogram(
    data_linkage,
    truncate_mode='lastp',
    p=nClust,
    leaf_rotation=90.,
    leaf_font_size=12.,
    show_contracted=True,
    annotate_above=10,
)
plt.show() #дендрограмма

#Иерархическая кластеризация
clusters=fcluster(data_linkage, nClust, criterion='maxclust')
clusters

col

x=0
y=1
plt.figure(figsize=(10, 8))
plt.scatter(dataNorm[:,x], dataNorm[:,y], c=clusters, cmap='flag')
plt.xlabel(col[x])
plt.ylabel(col[y]);
plt.show()

#к оригинальным данным добавляем номер кластера
df1['I']=clusters
res=df1.groupby('I')[col].mean()
res['Количество']=df1.groupby('I').size().values
res #средние цифры по кластерам и количество объектов (Количество)

res.to_csv('res.csv')

#Кластеризация методом KMeans
km = KMeans(n_clusters=nClust).fit(dataNorm)

#Вывод полученного распределение по кластерам
km.labels_ +1 #номер кластера, к котрому относится строка, так как нумерация начинается с нуля, выводим добавляя 1

x=1
y=0
centroids = km.cluster_centers_
plt.figure(figsize=(10, 8))
plt.scatter(dataNorm[:,x], dataNorm[:,y], c=km.labels_, cmap='flag')
plt.scatter(centroids[:, x], centroids[:, y], marker='*', s=300,
            c='b', label='centroid')
plt.xlabel(col[x])
plt.ylabel(col[y]);
plt.show()

# к оригинальным данным добавляем номера кластеров
df1['KMeans']=km.labels_+1
res=df1.groupby('KMeans')[col].mean()
res['Количество']=df1.groupby('KMeans').size().values
res

res.to_csv('res1.csv')

# Commented out IPython magic to ensure Python compatibility.
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# %matplotlib inline

dataset1=df1[['x','y','z', 'price','cut']]

features = ['x','y','z','price']
x = df.loc[:, features].values

y = df.loc[:,['cut']].values

x = StandardScaler().fit_transform(x)

pd.DataFrame(data = x, columns = features).head()

pca = PCA(n_components=2)

principalComponents = pca.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents,
                           columns = ['principal component 1', 'principal component 2'])

principalDf.head(5)

df[['cut']].head()

finalDf = pd.concat([principalDf, df[['cut']]], axis = 1)
finalDf.head(5)

fig = plt.figure(figsize = (50,50))
ax = fig.add_subplot(1,1,1)
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA', fontsize = 20)


targets = ['Ideal', 'Premium', 'Good', 'Very Good','Fair']
colors = ['r', 'g', 'b', 'y','violet']
for target, color in zip(targets,colors):
    indicesToKeep = finalDf['cut'] == target
    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']
               , finalDf.loc[indicesToKeep, 'principal component 2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()

pca.explained_variance_ratio_

#NMF
from sklearn.decomposition import NMF

dataset2=df1[['x','y','z', 'price']]

X = np.array(dataset2)
X

model=NMF(n_components=3)

W=model.fit_transform(X)
W

H=model.components_
H

print(np.dot(W,H))

print('Original Matrix')
print(X)
print('---------------')
print('\n Factorized Matrix')
print(np.dot(W,H))

NMF=np.dot(W,H)
print(NMF)

print(NMF.shape)

plt.figure(figsize=(12, 4))

# Исходные данные
plt.subplot(1, 3, 1)
plt.scatter(X[:, 0], X[:, 1], c='b', edgecolors='k')
plt.title('Исходные данные')

# Компонента 1
plt.subplot(1, 3, 2)
plt.scatter(W[:, 0], W[:, 1], c='r', marker='x')
plt.title('Компонента 1')

# Компонента 2
plt.subplot(1, 3, 3)
plt.scatter(W[:, 0], W[:, 1], c='g', marker='o')
plt.title('Компонента 2')

plt.tight_layout()
plt.show()
